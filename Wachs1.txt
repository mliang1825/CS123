Focus on the pairs. 
Let's say you wanted to figure out if the average word length of a review is correlated with helpfulness. 
That's not too complicated. 
Look at pairs of reviews, much more computationally expensive. 

Look at every pair of people who have left reviews, see how much in common. 
Or look at every pair of reviews for the same product. 

Fake reviews on Amazon, people paid to make fake reviews (reliable, lasts a long time, color, cheap). If all reviews
have same set of phrases, that's pretty suspicious. 

Look at pairs of reviewers, use really similar words or phrases. Do reviewers have similar style in all of the reviews they write? 
Look at how people use language. 
Need to have some justification for why you would need to use pairs to investigate a hypothesis. e.g. helpfulness vs. number of words it may not
be necessary. 

Reviews might be spread out across different versions of the same product. 

How many hypotheses do we need if we look at pairs of reviews? 3 or 4, or something in the same ballpark. 

These two reviews, similar words from diff users in diff cities, maybe the same person ultimately. 
Why is Amazon being so negligent about removing duplicate fake reviews? 

1. Pair reviewers, see how similar people are to each other in terms of the language they use. Pairing similar reviewers together. 
"Hey, you've left 12 reviews on Amazon; here's this other person who's left similar language who's interested in these other things, you might be 
interested in those things too."
2. Maybe people are so close to each other that they're the same person? 

By Week 7, want to have written the program for a subset of the data. 
If it turns out that pairing all the products doesn't take a long time. 

Try writing regular code to pair all the products together. If it you start on it and it takes an hour, then it's worthwhile to try a big data analysis. 